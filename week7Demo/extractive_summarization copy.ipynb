{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Text Summarization\n",
    "Extractive methods of text summarization try to summarize a document by selecting a subset of sentences which retain the most important points in the document.  In this notebook we will apply the [TextRank method](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) to extract the most important sentences in the document as the summary.  We build a graph from the sentences in the document and use the [PageRank algorithm](https://en.wikipedia.org/wiki/PageRank) to select the most central sentences in the document, which should also be the most important sentences in the document.  To create the graph from the document, we calculate the cosine similarity of each sentence with every other in the document and create a similarity matrix.  The similarity represents the weight of the edge between every pair of sentences.  The intuition is that the sentences which are most \"connected\" to the maximum number of other sentences in the document should be the most important.\n",
    "\n",
    "In order to calculate the similarity between sentences in a document, we need to create vectors representing each sentence. There are many ways we can do this - in this notebook we will demonstrate text summarization using Count Vectorization and TF-IDF Vectorization to create the text feature vectors.\n",
    "\n",
    "**Notes:**  \n",
    "- This does not need to be run on GPU for smaller documents such as articles\n",
    "\n",
    "**References:**  \n",
    "- Read the [TextRank paper](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) by Mihalcea and Tarau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document to summarize\n",
    "We will use BeautifulSoup to get the content of an article on the web and strip the text content from the hmtl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "url = 'https://en.wikipedia.org/wiki/Linear_regression'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Extract body text from article\n",
    "bodytext = soup.find_all('p')\n",
    "bodytext = [i.text for i in bodytext]\n",
    "article_text = ' '.join(bodytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text\n",
    "We will use some simple pre-processing on our document text:  \n",
    "- Split the text into sentences\n",
    "- Remove non-alphanumeric characters and stopwords from each sentence  \n",
    "- Separate sentences into lists of lower-case words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sents):\n",
    "    sentences_processed = []\n",
    "    for sentence in sents:\n",
    "        sentence_reduced = sentence.replace(\"[^a-zA-Z0-9_]\", '')\n",
    "        sentence_reduced = [word.lower() for word in sentence_reduced.split(' ') if word.lower() not in stopwords.words('english')]\n",
    "        sentences_processed.append(' '.join(word for word in sentence_reduced))\n",
    "    return sentences_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features using word counts or TFIDF\n",
    "Now we are ready to create our features.  We will first use simple word counts to create a numeric feature vector for each sentence.  Rather than using the Scikit-learn convenience method, we'll do this from scratch to demonstrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentences, vectorizer_type='count'):\n",
    "    if vectorizer_type == 'count':\n",
    "        # Get vocabulary for entire document\n",
    "        sentences = [sent.split(' ') for sent in sentences]\n",
    "        all_words = list(set([word for s in sentences for word in s]))\n",
    "\n",
    "        # Create feature vector for each sentence\n",
    "        feature_vecs = []\n",
    "        for sentence in sentences:\n",
    "            feature_vec = [0] * len(all_words)\n",
    "            for word in sentence:\n",
    "                feature_vec[all_words.index(word)] += 1\n",
    "            feature_vecs.append(feature_vec)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        feature_vecs = vectorizer.fit_transform(sentences)\n",
    "        feature_vecs = feature_vecs.todense().tolist()\n",
    "        \n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph representing document\n",
    "We will now convert our document, represented by sentence feature vectors, into a graph representing the document.  The nodes of the graph are the sentences, and the edges connecting the nodes represent the similarity of each sentence to every other.  To generate the graph we will create an adjacency matrix which stores the similarity values between every pair of sentences in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacency_matrix(feature_vecs):\n",
    "    # Create empty adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(feature_vecs), len(feature_vecs)))\n",
    " \n",
    "    # Populate the adjacency matrix using the similarity of all pairs of sentences\n",
    "    for i in range(len(feature_vecs)):\n",
    "        for j in range(len(feature_vecs)):\n",
    "            if i == j: #ignore if both are the same sentence\n",
    "                continue \n",
    "            adjacency_matrix[i][j] = 1 - cosine_distance(feature_vecs[1], feature_vecs[j])\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PageRank to get most important sentences\n",
    "Now that we have generated a graph representing the document, we can apply the PageRank algorithm to identify the most important sentences in the document as the most central nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sentences,adjacency_matrix,top_n):\n",
    "\n",
    "    # Create the graph representing the document\n",
    "    document_graph = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "    # Apply PageRank algorithm to get centrality scores for each node/sentence\n",
    "    scores = nx.pagerank(document_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Sort and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentences = [sentences[i] for i in ranking_idx]   \n",
    "\n",
    "    summary = []\n",
    "    for i in range(top_n):\n",
    "        summary.append(ranked_sentences[i])\n",
    "\n",
    "    summary = \" \".join(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the summarizer\n",
    "We've created all the components we need, now let's try it out on our example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. Such models are called linear models. Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. These are not the same as multivariable linear models (also called \"multiple linear models\"). \"General linear models\" are also called \"multivariate linear models\".\n"
     ]
    }
   ],
   "source": [
    "sentences_extracted = get_sentences(article_text)\n",
    "sentences_processed = preprocess(sentences_extracted)\n",
    "# Can set vectorizer_type = 'count' or 'tfidf' to change vectorizer type\n",
    "feature_vecs = vectorize(sentences_processed,vectorizer_type='count')\n",
    "adjacency_matrix = generate_adjacency_matrix(feature_vecs)\n",
    "summary = summarize(sentences_extracted,adjacency_matrix,top_n=5)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
